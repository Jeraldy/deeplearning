{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n",
    "\n",
    "**Exercise**: Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Loss =  0.7135990346948016\n",
      "[[0 0 0 1]] === [[0.29550276 0.29202402 0.29783112 0.29434383]]\n",
      "=======================================\n",
      "Loss =  0.5988161747148047\n",
      "[[0 0 0 1]] === [[0.17387406 0.2448307  0.2491679  0.33759626]]\n",
      "=======================================\n",
      "Loss =  0.5577531479030945\n",
      "[[0 0 0 1]] === [[0.125802   0.23623601 0.2397158  0.40195564]]\n",
      "=======================================\n",
      "Loss =  0.5435489496253247\n",
      "[[0 0 0 1]] === [[0.09152545 0.22563662 0.22836479 0.45704237]]\n",
      "=======================================\n",
      "Loss =  0.5428776442021304\n",
      "[[0 0 0 1]] === [[0.06693312 0.21410997 0.21617359 0.50454299]]\n",
      "=======================================\n",
      "Loss =  0.5496013017444414\n",
      "[[0 0 0 1]] === [[0.04986207 0.20280469 0.20431198 0.54415989]]\n",
      "=======================================\n",
      "Loss =  0.5600653818053128\n",
      "[[0 0 0 1]] === [[0.03828546 0.19256645 0.19363563 0.57614871]]\n",
      "=======================================\n",
      "Loss =  0.571978062061931\n",
      "[[0 0 0 1]] === [[0.03047377 0.18379608 0.18453673 0.60141174]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "m = X.shape[0]\n",
    "lambd = 0.1\n",
    "num_nodes = 400\n",
    "\n",
    "W1 = np.random.randn(num_nodes,X.shape[1])*0.1\n",
    "b1 = np.zeros((num_nodes,1))\n",
    "\n",
    "W2 = np.random.randn(1,num_nodes)*0.1\n",
    "b2 = np.zeros((1,X.shape[0]))\n",
    "\n",
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "costs = []\n",
    "\n",
    "for i in range(4000):\n",
    "    # Foward Prop\n",
    "    # LAYER 1\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = 1/(1+np.exp(-Z1))\n",
    "    # LAYER 2\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = 1/(1+np.exp(-Z2))\n",
    "    \n",
    "    # Back Prop\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T) + (lambd * W2) / m # CHANGED\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T) + (lambd * W1) / m # CHANGED\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    W2 = W2 - 0.01*dW2\n",
    "    b2 = b2 - 0.01*db2\n",
    "    \n",
    "    W1 = W1 - 0.01*dW1\n",
    "    b1 = b1 - 0.01*db1\n",
    "    \n",
    "    # Loss\n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2))) / (2 * m) # ADDED\n",
    "    \n",
    "    L = (-1/m)*np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2))\n",
    "    L = np.squeeze(L) + L2_regularization_cost # CHANGED\n",
    "    costs.append(L)\n",
    "    if i%500 == 0:\n",
    "        print(\"=======================================\")\n",
    "        print(\"Loss = \",L)\n",
    "        print(Y,\"===\",A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

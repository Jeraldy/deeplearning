{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation with dropout\n",
    "\n",
    "**Exercise**: Implement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer. \n",
    "\n",
    "**Instructions**:\n",
    "You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps:\n",
    "1. In lecture, we dicussed creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using `np.random.rand()` to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ of the same dimension as $A^{[1]}$.\n",
    "2. Set each entry of $D^{[1]}$ to be 0 with probability (`1-keep_prob`) or 1 with probability (`keep_prob`), by thresholding values in $D^{[1]}$ appropriately. Hint: to set all the entries of a matrix X to 0 (if entry is less than 0.5) or 1 (if entry is more than 0.5) you would do: `X = (X < 0.5)`. Note that 0 and 1 are respectively equivalent to False and True.\n",
    "3. Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. (You are shutting down some neurons). You can think of $D^{[1]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.\n",
    "4. Divide $A^{[1]}$ by `keep_prob`. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Loss =  0.7333137291590309\n",
      "[[0 0 0 1]] === [[0.33211028 0.48670396 0.78340635 0.71677886]]\n",
      "=======================================\n",
      "Loss =  0.389660193981372\n",
      "[[0 0 0 1]] === [[0.08466573 0.36032529 0.16109699 0.42839092]]\n",
      "=======================================\n",
      "Loss =  0.503711467174047\n",
      "[[0 0 0 1]] === [[0.11836645 0.3867019  0.28488285 0.34484715]]\n",
      "=======================================\n",
      "Loss =  0.38039513573676753\n",
      "[[0 0 0 1]] === [[0.05191123 0.44623436 0.22913318 0.53954988]]\n",
      "=======================================\n",
      "Loss =  0.26417223114916083\n",
      "[[0 0 0 1]] === [[0.09689711 0.12925555 0.2825547  0.61612537]]\n",
      "=======================================\n",
      "Loss =  0.15357637721061695\n",
      "[[0 0 0 1]] === [[0.04646957 0.12845001 0.08485497 0.71136702]]\n",
      "=======================================\n",
      "Loss =  0.1557863015917216\n",
      "[[0 0 0 1]] === [[0.00136849 0.18256184 0.04861382 0.69048535]]\n",
      "=======================================\n",
      "Loss =  0.12344912810594039\n",
      "[[0 0 0 1]] === [[0.04198774 0.29365151 0.00955837 0.91060058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "m = X.shape[0]\n",
    "keep_prob = 0.6\n",
    "num_nodes = 400\n",
    "\n",
    "W1 = np.random.randn(num_nodes,X.shape[1])*0.1\n",
    "b1 = np.zeros((num_nodes,1))\n",
    "\n",
    "W2 = np.random.randn(1,num_nodes)*0.1\n",
    "b2 = np.zeros((1,X.shape[0]))\n",
    "\n",
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "costs = []\n",
    "\n",
    "for i in range(4000):\n",
    "    # Foward Prop\n",
    "    # LAYER 1\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = 1/(1+np.exp(-Z1))\n",
    "    \n",
    "    # Apply Drop Out to the Final Layer\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
    "    D1 = D1 < keep_prob\n",
    "    A1 = A1 * D1\n",
    "    A1 = A1 / keep_prob\n",
    "    \n",
    "    # LAYER 2\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = 1/(1+np.exp(-Z2))\n",
    "    \n",
    "    # Back Prop\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    W2 = W2 - 0.01*dW2\n",
    "    b2 = b2 - 0.01*db2\n",
    "    \n",
    "    W1 = W1 - 0.01*dW1\n",
    "    b1 = b1 - 0.01*db1\n",
    "    \n",
    "    # Loss \n",
    "    L = (-1/m)*np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2))\n",
    "    L = np.squeeze(L)\n",
    "    costs.append(L)\n",
    "    if i%500 == 0:\n",
    "        print(\"=======================================\")\n",
    "        print(\"Loss = \",L)\n",
    "        print(Y,\"===\",A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
